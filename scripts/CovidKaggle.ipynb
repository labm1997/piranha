{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install keras\n",
        "!pip install pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUK7uoflTKKQ",
        "outputId": "6ff815d3-b435-40c8-9a39-8b3df18bb893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq-egIcyTAR7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import math\n",
        "import random\n",
        "import sklearn as sk\n",
        "import seaborn as sns\n",
        "import matplotlib.colors as mcolors\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models             import Model, Sequential\n",
        "from keras.models             import model_from_json\n",
        "from keras.layers             import *\n",
        "from keras.utils.vis_utils    import plot_model\n",
        "from sklearn.metrics          import accuracy_score, confusion_matrix,classification_report\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce"
      ],
      "metadata": {
        "id": "1iG6rOyNUoMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSOVdwNTTR6Y",
        "outputId": "69f31ec9-18e4-43f7-9232-67aa19da87d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Converts to grayscale images\n",
        "# a = [\"/content/drive/MyDrive/dataset_kaggle/Test\",\"/content/drive/MyDrive/dataset_kaggle/Train\"]\n",
        "# b = [\"/content/drive/MyDrive/dataset_kaggle_grayscale/Test\",\"/content/drive/MyDrive/dataset_kaggle_grayscale/Train\"]\n",
        "# for (o,d) in zip(a,b):\n",
        "#     classes_dirs = [e for e in os.listdir(o) if e != \".DS_Store\"]\n",
        "#     for class_dir in classes_dirs:\n",
        "#       for filename in os.listdir(os.path.join(o, class_dir)):\n",
        "#         input_filename = os.path.join(o, class_dir, filename)\n",
        "\n",
        "#         image = cv2.imread(input_filename)\n",
        "#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#         output_filename = os.path.join(d, class_dir, filename)\n",
        "\n",
        "#         os.makedirs(os.path.join(d, class_dir), exist_ok=True)\n",
        "#         cv2.imwrite(output_filename, gray)\n"
      ],
      "metadata": {
        "id": "-awB1IuB-RmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pasta_raiz='/content/drive/MyDrive/dataset_kaggle_grayscale/'\n",
        "dirtreino=pasta_raiz+\"/Train/\"\n",
        "dirteste=pasta_raiz+\"/Test/\"\n",
        "dirvalidacao=pasta_raiz+\"/Validation/\"\n",
        "dirmodelos=pasta_raiz+\"/Models/\""
      ],
      "metadata": {
        "id": "MHE4FY99DCnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pesos_ref = 'imagenet'\n",
        "num_classes = 4\n",
        "tamanho_lote_treino = 16\n",
        "tamanho_lote_teste = 8\n",
        "perc_dropout = 0.3\n",
        "epocas=50"
      ],
      "metadata": {
        "id": "JEQ49r4kTWUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessador = tf.keras.applications.xception.preprocess_input\n",
        "tamanho_imagem = 224"
      ],
      "metadata": {
        "id": "cJ4cr5cETpfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation of images into TensorFlow format. The ImageDataGenerator class imports the files from the directory and allows the generation of images,\n",
        "# allowing change of rotation, height, width and zoom.\n",
        "# With the preprocessing function there is a scale transformation of values, for example from 0 to 255 to 0 and 1.\n",
        "# The pre-existing Neural Network structure to be used by the pre-processing function is indicated\n",
        "# imagens_treino_gerador_dados = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessador,\n",
        "imagens_treino_gerador_dados = ImageDataGenerator(preprocessing_function=preprocessador,\n",
        "                                                 rotation_range = 50, # image rotations\n",
        "                                                 #width_shift_range = 0.2, # image width change\n",
        "                                                 #height_shift_range = 0.2, # image height change\n",
        "                                                 zoom_range = 0.1, # image zoom change\n",
        "                                                 horizontal_flip = True, # horizontal rotations\n",
        "                                                 vertical_flip = True) # vertical rotations"
      ],
      "metadata": {
        "id": "k-XWMGckTdOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for images from the directory, performs preprocessing with normalization\n",
        "tamanho_lote_treino = 50\n",
        "gerador_treino = imagens_treino_gerador_dados.flow_from_directory(dirtreino,\n",
        "                                                                          target_size=(tamanho_imagem,tamanho_imagem),\n",
        "                                                                          batch_size=tamanho_lote_treino,\n",
        "                                                                          class_mode='sparse',\n",
        "                                                                          color_mode='grayscale',\n",
        "                                                                          shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjavjHYmUHC4",
        "outputId": "a3946c36-a7f0-47ad-b28a-cf131c3647c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # imagens_teste_gerador_dados = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessador)\n",
        "# imagens_teste_gerador_dados = ImageDataGenerator(preprocessing_function=preprocessador)\n",
        "# gerador_teste = imagens_teste_gerador_dados.flow_from_directory(dirteste,\n",
        "#                                                                         target_size=(tamanho_imagem,tamanho_imagem), # images size for resizing\n",
        "#                                                                         batch_size=tamanho_lote_teste , # Image batch size to be used by the Neural Network for error calculation. The error found will be used for weight adjustments.\n",
        "#                                                                         class_mode='sparse',\n",
        "#                                                                         color_mode='grayscale',\n",
        "#                                                                         shuffle=False) # Randomly searched images\n",
        "x_test = []\n",
        "y_test = []\n",
        "for test_class_dir in os.listdir(dirteste):\n",
        "  for filename in os.listdir(os.path.join(dirteste, test_class_dir)):\n",
        "    # print(\"reading\", filename)\n",
        "    raw_image = cv2.imread(os.path.join(dirteste, test_class_dir, filename), cv2.IMREAD_UNCHANGED)\n",
        "    pp_image = preprocessador(raw_image)\n",
        "    resized_image = cv2.resize(pp_image, (tamanho_imagem, tamanho_imagem))\n",
        "    x_test.append(resized_image)\n",
        "    y_test.append(gerador_treino.class_indices[test_class_dir])\n",
        "x_test = np.expand_dims(np.array(x_test), 3)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "9Qs2PV5bU1iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(tamanho_imagem,tamanho_imagem,1), use_bias=False))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3,3), activation='relu', use_bias=False))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', use_bias=False))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', use_bias=False))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "WiUwME5TVkf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81976abb-538e-49a8-cdd9-3047d3fd7a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 222, 222, 64)      576       \n",
            "                                                                 \n",
            " average_pooling2d_8 (Averag  (None, 111, 111, 64)     0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 109, 109, 64)      36864     \n",
            "                                                                 \n",
            " average_pooling2d_9 (Averag  (None, 54, 54, 64)       0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 52, 52, 128)       73728     \n",
            "                                                                 \n",
            " average_pooling2d_10 (Avera  (None, 26, 26, 128)      0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 24, 24, 128)       147456    \n",
            "                                                                 \n",
            " average_pooling2d_11 (Avera  (None, 12, 12, 128)      0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 18432)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               2359424   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,635,076\n",
            "Trainable params: 2,635,076\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AZLSWqC7WBAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit_generator allows you to search for the images and carry out the training\n",
        "historico_epocas = model.fit_generator(generator=gerador_treino,\n",
        "                                                   epochs=epocas,\n",
        "                                                   use_multiprocessing=True\n",
        "                                                   ,steps_per_epoch = gerador_treino.n // gerador_treino.batch_size\n",
        "                                                   ,validation_data = (x_test, y_test)\n",
        "                                                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lSbbZFUVzJa",
        "outputId": "d6757f4e-b94e-4a04-c238-3b3eee29dc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-86377c349c93>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  historico_epocas = model.fit_generator(generator=gerador_treino,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 8s 433ms/step - loss: 1.3837 - accuracy: 0.2975 - val_loss: 1.3476 - val_accuracy: 0.2976\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 6s 615ms/step - loss: 1.2282 - accuracy: 0.4700 - val_loss: 1.4523 - val_accuracy: 0.3810\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 5s 579ms/step - loss: 1.2002 - accuracy: 0.4275 - val_loss: 1.2742 - val_accuracy: 0.3929\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 5s 586ms/step - loss: 1.0821 - accuracy: 0.5450 - val_loss: 1.3218 - val_accuracy: 0.5476\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 5s 633ms/step - loss: 1.0181 - accuracy: 0.5450 - val_loss: 1.2666 - val_accuracy: 0.4881\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 6s 665ms/step - loss: 0.9883 - accuracy: 0.5600 - val_loss: 1.0538 - val_accuracy: 0.5714\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 6s 722ms/step - loss: 0.9821 - accuracy: 0.5825 - val_loss: 1.0787 - val_accuracy: 0.5119\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 6s 706ms/step - loss: 0.9284 - accuracy: 0.6200 - val_loss: 1.2978 - val_accuracy: 0.5595\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 5s 554ms/step - loss: 0.9638 - accuracy: 0.5925 - val_loss: 1.0620 - val_accuracy: 0.5119\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 8s 955ms/step - loss: 0.9485 - accuracy: 0.5850 - val_loss: 1.0273 - val_accuracy: 0.5714\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 6s 713ms/step - loss: 0.8687 - accuracy: 0.6300 - val_loss: 1.0975 - val_accuracy: 0.5119\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 6s 736ms/step - loss: 0.8590 - accuracy: 0.6375 - val_loss: 1.1409 - val_accuracy: 0.5476\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 7s 849ms/step - loss: 0.8608 - accuracy: 0.6200 - val_loss: 0.9465 - val_accuracy: 0.6071\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 5s 578ms/step - loss: 0.8476 - accuracy: 0.6450 - val_loss: 1.0911 - val_accuracy: 0.5595\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.7892 - accuracy: 0.6625 - val_loss: 0.9373 - val_accuracy: 0.5595\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 5s 636ms/step - loss: 0.8127 - accuracy: 0.6475 - val_loss: 1.1230 - val_accuracy: 0.4881\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 7s 704ms/step - loss: 0.7694 - accuracy: 0.6650 - val_loss: 1.1931 - val_accuracy: 0.5714\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 5s 564ms/step - loss: 0.7725 - accuracy: 0.6825 - val_loss: 1.2537 - val_accuracy: 0.4524\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 8s 902ms/step - loss: 0.8394 - accuracy: 0.6650 - val_loss: 1.0668 - val_accuracy: 0.6071\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 6s 714ms/step - loss: 0.7869 - accuracy: 0.6950 - val_loss: 1.1136 - val_accuracy: 0.4286\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 8s 936ms/step - loss: 0.7676 - accuracy: 0.6850 - val_loss: 1.2307 - val_accuracy: 0.5714\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 7s 730ms/step - loss: 0.7235 - accuracy: 0.6875 - val_loss: 1.3120 - val_accuracy: 0.5476\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 5s 618ms/step - loss: 0.7982 - accuracy: 0.6750 - val_loss: 0.9070 - val_accuracy: 0.6310\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 7s 850ms/step - loss: 0.7858 - accuracy: 0.6525 - val_loss: 1.0295 - val_accuracy: 0.5476\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 7s 713ms/step - loss: 0.7282 - accuracy: 0.6850 - val_loss: 1.0039 - val_accuracy: 0.5357\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 5s 548ms/step - loss: 0.7587 - accuracy: 0.6950 - val_loss: 1.0384 - val_accuracy: 0.5952\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 5s 539ms/step - loss: 0.7132 - accuracy: 0.7350 - val_loss: 1.1700 - val_accuracy: 0.5476\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 5s 646ms/step - loss: 0.7142 - accuracy: 0.6950 - val_loss: 1.1662 - val_accuracy: 0.5357\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 7s 748ms/step - loss: 0.6811 - accuracy: 0.7150 - val_loss: 1.0767 - val_accuracy: 0.5714\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 5s 631ms/step - loss: 0.6499 - accuracy: 0.7300 - val_loss: 1.1957 - val_accuracy: 0.5476\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 6s 683ms/step - loss: 0.6565 - accuracy: 0.7100 - val_loss: 1.2674 - val_accuracy: 0.5952\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 6s 679ms/step - loss: 0.6505 - accuracy: 0.7125 - val_loss: 1.0361 - val_accuracy: 0.5833\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 6s 609ms/step - loss: 0.6533 - accuracy: 0.7300 - val_loss: 1.0241 - val_accuracy: 0.6071\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 6s 759ms/step - loss: 0.6685 - accuracy: 0.7275 - val_loss: 1.0352 - val_accuracy: 0.6548\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 6s 689ms/step - loss: 0.6107 - accuracy: 0.7525 - val_loss: 1.1154 - val_accuracy: 0.6429\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 6s 728ms/step - loss: 0.6393 - accuracy: 0.7250 - val_loss: 1.1505 - val_accuracy: 0.5952\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 7s 856ms/step - loss: 0.6894 - accuracy: 0.7275 - val_loss: 1.1076 - val_accuracy: 0.5952\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 8s 878ms/step - loss: 0.6181 - accuracy: 0.7475 - val_loss: 1.0898 - val_accuracy: 0.5952\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 5s 655ms/step - loss: 0.5836 - accuracy: 0.7350 - val_loss: 1.3537 - val_accuracy: 0.5238\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 6s 700ms/step - loss: 0.5635 - accuracy: 0.7625 - val_loss: 1.3182 - val_accuracy: 0.6310\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 8s 979ms/step - loss: 0.6316 - accuracy: 0.7475 - val_loss: 1.0760 - val_accuracy: 0.6667\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 6s 725ms/step - loss: 0.6274 - accuracy: 0.7575 - val_loss: 0.9492 - val_accuracy: 0.6667\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 6s 692ms/step - loss: 0.6016 - accuracy: 0.7800 - val_loss: 0.9284 - val_accuracy: 0.6786\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 5s 604ms/step - loss: 0.5143 - accuracy: 0.8000 - val_loss: 1.3489 - val_accuracy: 0.6310\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 7s 838ms/step - loss: 0.5668 - accuracy: 0.7925 - val_loss: 1.2599 - val_accuracy: 0.6071\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 6s 667ms/step - loss: 0.5891 - accuracy: 0.7775 - val_loss: 1.1098 - val_accuracy: 0.6071\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 6s 614ms/step - loss: 0.5961 - accuracy: 0.7600 - val_loss: 1.2088 - val_accuracy: 0.6310\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 6s 693ms/step - loss: 0.5810 - accuracy: 0.7525 - val_loss: 1.0228 - val_accuracy: 0.6429\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 5s 612ms/step - loss: 0.5200 - accuracy: 0.8025 - val_loss: 1.1154 - val_accuracy: 0.6310\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 8s 945ms/step - loss: 0.5261 - accuracy: 0.8050 - val_loss: 1.0752 - val_accuracy: 0.6310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "print(gerador_treino.class_indices)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyF4LgI4LLI5",
        "outputId": "ef390747-2e86-4892-e413-ca98f4299cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 28ms/step\n",
            "{'Covid': 0, 'Lung_Opacity': 1, 'Normal': 2, 'Viral Pneumonia': 3}\n",
            "[[ 7 11  1  2]\n",
            " [ 3 16  1  1]\n",
            " [ 4  8  9  0]\n",
            " [ 0  0  0 21]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.33      0.40        21\n",
            "           1       0.46      0.76      0.57        21\n",
            "           2       0.82      0.43      0.56        21\n",
            "           3       0.88      1.00      0.93        21\n",
            "\n",
            "    accuracy                           0.63        84\n",
            "   macro avg       0.66      0.63      0.62        84\n",
            "weighted avg       0.66      0.63      0.62        84\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(dirmodelos + \"/Maia-6\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvmbU2guSX5U",
        "outputId": "29c4288d-b696-497f-d531-2d38efbc96e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save(filename, data):\n",
        "    with open(filename, \"w\") as file:\n",
        "        for v in data.flatten():\n",
        "            file.write(str(v) + \"\\n\")"
      ],
      "metadata": {
        "id": "CCtM9PlEXUZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saves weights in piranha format\n",
        "piranha_weights_output_dir = dirmodelos + \"/Maia-6\" + \"/piranha\"\n",
        "os.makedirs(piranha_weights_output_dir, exist_ok=True)\n",
        "\n",
        "parameters = 0\n",
        "for i,layer in enumerate([l for l in model.layers if len(l.get_weights()) > 0]):\n",
        "  name = layer.name\n",
        "  all_weights = layer.get_weights()\n",
        "\n",
        "  # FC\n",
        "  if len(all_weights) == 2:\n",
        "    [weights, bias] = all_weights\n",
        "    save(os.path.join(piranha_weights_output_dir, \"weight\" + str(i)), weights)\n",
        "    save(os.path.join(piranha_weights_output_dir, \"bias\" + str(i)), bias)\n",
        "    parameters += weights.size + bias.size\n",
        "  #CNN\n",
        "  elif len(all_weights) == 1:\n",
        "    [weights] = all_weights\n",
        "    save(os.path.join(piranha_weights_output_dir, \"weight\" + str(i)), weights)\n",
        "    parameters += weights.size\n",
        "  else:\n",
        "    print(\"Not expected\", len(all_weights))\n",
        "\n",
        "\n",
        "print(\"parameters\", parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9ckn76LS8kt",
        "outputId": "9ce38b14-7a90-4a4e-c00a-cd1efec9e8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters 2635076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hot_encode(y):\n",
        "    return np.array([[1,0] if e == 0 else [0,1] for e in y])"
      ],
      "metadata": {
        "id": "jjiPLC87bW0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saves test dataset in piranha format\n",
        "piranha_test = pasta_raiz + \"/Piranha\" + \"/Test\"\n",
        "os.makedirs(piranha_test, exist_ok=True)\n",
        "\n",
        "save(os.path.join(piranha_test, \"test_data\"), x_test)\n",
        "save(os.path.join(piranha_test, \"test_labels\"), y_test)"
      ],
      "metadata": {
        "id": "CLeUKrvebMdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}